{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (3.10.0)\n",
      "Requirement already satisfied: numpy in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (2.2.1)\n",
      "Requirement already satisfied: opencv-python in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: scikit-image in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (0.25.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: openai in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (1.59.9)\n",
      "Requirement already satisfied: gym in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (0.26.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.11.2 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from scikit-image) (1.15.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from scikit-image) (3.4.2)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from scikit-image) (2.36.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from scikit-image) (2024.12.12)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from scikit-image) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from openai) (2.10.5)\n",
      "Requirement already satisfied: sniffio in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from gym) (3.1.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/anupamgadobe.com/Work/sam2/.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install matplotlib numpy opencv-python scikit-image scikit-learn openai gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "# from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import io\n",
    "import base64\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam2_checkpoint = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sam_model = build_sam2(model_cfg, sam2_checkpoint, device=device, apply_postprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authorization token\n",
    "auth_token = \"eyJhbGciOiJSUzI1NiIsIng1dSI6Imltc19uYTEtc3RnMS1rZXktYXQtMS5jZXIiLCJraWQiOiJpbXNfbmExLXN0ZzEta2V5LWF0LTEiLCJpdHQiOiJhdCJ9.eyJpZCI6IjE3MzY4Nzc4NDY4MjFfZTBmYjVmMmYtZWVmYy00ZGVmLWI2ZTktODkwNmQzNGQyZDU0X3V3MiIsInR5cGUiOiJhY2Nlc3NfdG9rZW4iLCJjbGllbnRfaWQiOiJJRENsb3VkU2VydmljZTIiLCJ1c2VyX2lkIjoiSURDbG91ZFNlcnZpY2UyQEFkb2JlSUQiLCJhcyI6Imltcy1uYTEtc3RnMSIsImFhX2lkIjoiSURDbG91ZFNlcnZpY2UyQEFkb2JlSUQiLCJjdHAiOjAsInBhYyI6IklEQ2xvdWRTZXJ2aWNlMl9zdGcyIiwicnRpZCI6IjE3MzY4Nzc4NDY4MjFfZTk1NWFiMmUtMmZhYy00NTdhLTg4YmUtZGRhNmQ0NjEzMTgzX3V3MiIsIm1vaSI6ImQyNWYxZmM2IiwicnRlYSI6IjE3MzgwODc0NDY4MjEiLCJleHBpcmVzX2luIjoiODY0MDAwMDAiLCJzY29wZSI6InN5c3RlbSxvcGVuaWQsYWRkaXRpb25hbF9pbmZvLm93bmVyT3JnLGFkbWluX21hbmFnZV93b3JrZmxvd3MsaW5kZXNpZ25fc2VydmljZXMiLCJjcmVhdGVkX2F0IjoiMTczNjg3Nzg0NjgyMSJ9.OkXCRGjzOigH6l5MsiXJQZMs5-SnU4o12nTC_TlhXxuN0j26EwfL2AA2w-LKCPa46TECH8ni2VNwChY9hCUaxDy3TglfGtT5_LVjdiydxDtn2XFOljLLL0KMMYflUWEy9GcoJgjMw86o1_LTxL9eCqNroeXHoOdowM_Bhfg1NX1S2rpmU1An4qtYS25nSU2yX1YkS_Uw04LCrELD4d3WnWjEJKI4elHsVhkoG8seE64JA2LPCks2Q2EXy8PYaqv064Z2OQPOq8zV00-J4GCaLI51FUZab13j5kTIUHqVZ32fY4Da9ydh2ipgFhfBhCwMQF5H4rZ9b4WVrNL7L7zidQ\"\n",
    "\n",
    "# Azure OpenAI API key\n",
    "azure_openai_api_key = \"FkTtgcdo82dJwR4h23hM1bZ9cahYfHeOkH4rFLIfxqOmLWr7unlkJQQJ99BAACYeBjFXJ3w3AAABACOGacz3\"  # Replace with your Azure OpenAI API key\n",
    "\n",
    "# Azure OpenAI API endpoint\n",
    "azure_openai_api_endpoint = \"https://gpt-ml.openai.azure.com/\"\n",
    "\n",
    "# Azure OpenAI model name = \"gpt-4o\"\n",
    "azure_openai_model_name = \"gpt-4o\"\n",
    "\n",
    "# Azure OpenAI model version = \"2024-10-01-preview\"\n",
    "azure_openai_model_version = \"2024-10-01-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Azure OpenAI client\n",
    "azure_openai_client = AzureOpenAI(  \n",
    "    azure_endpoint=azure_openai_api_endpoint,  \n",
    "    api_key=azure_openai_api_key,  \n",
    "    api_version=azure_openai_model_version,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Async function to encode a local image into data URL\n",
    "async def local_image_bytes_to_data_url(image_bytes):\n",
    "    # Guess the MIME type of the image based on the file extension\n",
    "    mime_type = Image.open(io.BytesIO(image_bytes)).get_format_mimetype()\n",
    "    if mime_type is None:\n",
    "        mime_type = 'application/octet-stream'  # Default MIME type if none is found\n",
    "\n",
    "    base64_encoded_data = base64.b64encode(image_bytes).decode('utf-8')\n",
    "\n",
    "    # Construct the data URL\n",
    "    return f\"data:{mime_type};base64,{base64_encoded_data}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async method for extracting json from text json response\n",
    "async def extract_json_from_response(extracted_text_resp):\n",
    "    try:\n",
    "        response_json = json.loads(extracted_text_resp)\n",
    "        return response_json\n",
    "    except ValueError:\n",
    "        print(f\"extracted_text_resp is not a valid JSON string\")\n",
    "        pass\n",
    "\n",
    "    # Extract the JSON from the response\n",
    "    pattern = r'(.*?)```json(.*?)```(.*)'\n",
    "\n",
    "    # Search for the pattern in the input string using re.DOTALL to include newlines\n",
    "    match = re.search(pattern, extracted_text_resp, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        # Extract the JSON content and strip leading/trailing whitespace\n",
    "        json_str_resp = match.group(2).strip()\n",
    "        response_json = json.loads(json_str_resp)\n",
    "        # print(json.dumps(response_json, indent=4))\n",
    "        return response_json\n",
    "    else:\n",
    "        # Raise an error if the pattern is not found\n",
    "        raise ValueError(\"JSON content not found in the input string.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async method for detecting background elements in an image with gpt-4-vision api\n",
    "async def detect_background_elements(image_data_url):\n",
    "    # Sample output JSON for background elements\n",
    "    output_json = {\n",
    "        \"image_size\": {\n",
    "            \"width\": \"<Width of the bounding box>\",\n",
    "            \"height\": \"<Height of the bounding box>\"\n",
    "        },\n",
    "        \"background_elements\": [\n",
    "            {\n",
    "                \"type\": \"filled-rectangle\",\n",
    "                \"bounding_box\": {\n",
    "                    \"x\": \"<X-coordinate of the top-left corner>\",\n",
    "                    \"y\": \"<Y-coordinate of the top-left corner>\",\n",
    "                    \"width\": \"<Width of the bounding box>\",\n",
    "                    \"height\": \"<Height of the bounding box>\"\n",
    "                },\n",
    "                \"fillColor\": \"#87CEEB\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    background_extraction_prompt = \"\"\"\n",
    "    **Task Description:**  \n",
    "    Analyze the provided image and extract all background details as structured data. \n",
    "    The goal is to create a JSON representation of the background that can be used to redraw it on a white canvas. \n",
    "    Focus on identifying backgrounds, patterns, shapes, colors, and gradients, ignoring any foreground elements.\n",
    "\n",
    "    **Steps:**  \n",
    "    1. Examine the input image and locate all background components.  \n",
    "    2. Distinguish between different types of background elements (solid colors, gradients, shapes, patterns).\n",
    "    3. background element should not contain any text related object. \n",
    "    3. For each element, record its type, bounding box(x, y, width, height), color(s), and any other relevant attributes.  \n",
    "    4. Compile these details into a structured JSON object along with the image size that can be used to reconstruct the background on a blank canvas.\n",
    "    5. Provide only the structured JSON representation of the background, without any extra details or text information.\n",
    "\n",
    "    **Input Image:** \n",
    "\n",
    "    Attached in image_url\n",
    "\n",
    "    **Expected Output Example:**\n",
    "    Respond only in the following JSON format:\n",
    "\n",
    "    {output_json}\n",
    "    \"\"\"\n",
    "\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": background_extraction_prompt\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": image_data_url\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    chat_prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"You are an AI assistant that helps people find information.\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        user_message\n",
    "    ]\n",
    "\n",
    "    # Generate the completion  \n",
    "    # make a request to the gpt-4o api\n",
    "    completion = azure_openai_client.chat.completions.create(  \n",
    "        model=azure_openai_model_name,  \n",
    "        messages=chat_prompt,  \n",
    "        max_tokens=2800,  \n",
    "        temperature=0.1,  \n",
    "        top_p=0.9,  \n",
    "        frequency_penalty=0,  \n",
    "        presence_penalty=0,  \n",
    "        stop=None,  \n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    response = (completion.to_json())\n",
    "    extracted_text_resp = json.loads(response)['choices'][0]['message']['content']\n",
    "    print(f\"Extracted text response for background elements: {extracted_text_resp}\")\n",
    "    background_elements_json = await extract_json_from_response(extracted_text_resp)\n",
    "    print(f\"Background elements JSON: {background_elements_json}\")\n",
    "    return background_elements_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_background_elements(bg_elements, new_width, new_height):\n",
    "    curr_img_size = bg_elements[\"image_size\"]\n",
    "    original_width = curr_img_size[\"width\"]\n",
    "    original_height = curr_img_size[\"height\"]   \n",
    "    width_ratio = new_width / original_width\n",
    "    height_ratio = new_height / original_height\n",
    "\n",
    "    adjust_background_elements = {\n",
    "        \"image_size\": {\n",
    "            \"width\": new_width,\n",
    "            \"height\": new_height\n",
    "        },\n",
    "        \"background_elements\": []\n",
    "    }\n",
    "\n",
    "    adjusted_elements = []\n",
    "    for element in bg_elements[\"background_elements\"]:\n",
    "        print(f\"Element: {element}\")\n",
    "        if \"color\" not in element:\n",
    "            continue\n",
    "        adjusted_bounding_box = {\n",
    "            \"x\": element[\"bounding_box\"][\"x\"] * width_ratio,\n",
    "            \"y\": element[\"bounding_box\"][\"y\"] * height_ratio,\n",
    "            \"width\": element[\"bounding_box\"][\"width\"] * width_ratio,\n",
    "            \"height\": element[\"bounding_box\"][\"height\"] * height_ratio\n",
    "        }\n",
    "        adjusted_element = {\n",
    "            \"color\": element[\"color\"],\n",
    "            \"type\": element[\"type\"],\n",
    "            \"bounding_box\": adjusted_bounding_box\n",
    "        }\n",
    "        adjusted_elements.append(adjusted_element)\n",
    "    \n",
    "    adjust_background_elements[\"background_elements\"] = adjusted_elements\n",
    "    return adjust_background_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for drawing background elements on a canvas\n",
    "def draw_background_elements(canvas, background_elements, image_size):\n",
    "    # Create a new image with the specified size\n",
    "    canvas = Image.new(\"RGB\", (image_size[\"width\"], image_size[\"height\"]), (255, 255, 255))\n",
    "    # Create a drawing object\n",
    "    draw = ImageDraw.Draw(canvas)\n",
    "    print(f\"background_elements: {background_elements}\")\n",
    "    for element in background_elements[\"background_elements\"]:\n",
    "        print(f\"Element: {element}\")\n",
    "        print(f\"Element type: {element[\"type\"]}\")\n",
    "        if element[\"type\"] in [\"filled-rectangle\", \"solid_color\", \"rectangle\"]:\n",
    "            # Extract the position, size, and color of the filled rectangle\n",
    "            x = element[\"bounding_box\"][\"x\"]\n",
    "            y = element[\"bounding_box\"][\"y\"]\n",
    "            width = element[\"bounding_box\"][\"width\"]\n",
    "            height = element[\"bounding_box\"][\"height\"]\n",
    "            fill_color = element[\"color\"]\n",
    "            draw.rectangle([x, y, x + width, y + height], fill=fill_color)\n",
    "    canvas.show()\n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async method for extracting json from text json response\n",
    "async def extract_json_from_response(extracted_text_resp):\n",
    "    try:\n",
    "        response_json = json.loads(extracted_text_resp)\n",
    "        return response_json\n",
    "    except ValueError:\n",
    "        print(f\"extracted_text_resp is not a valid JSON string\")\n",
    "        pass\n",
    "\n",
    "    # Extract the JSON from the response\n",
    "    pattern = r'(.*?)```json(.*?)```(.*)'\n",
    "\n",
    "    # Search for the pattern in the input string using re.DOTALL to include newlines\n",
    "    match = re.search(pattern, extracted_text_resp, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        # Extract the JSON content and strip leading/trailing whitespace\n",
    "        json_str_resp = match.group(2).strip()\n",
    "        response_json = json.loads(json_str_resp)\n",
    "        # print(json.dumps(response_json, indent=4))\n",
    "        return response_json\n",
    "    else:\n",
    "        # Raise an error if the pattern is not found\n",
    "        raise ValueError(\"JSON content not found in the input string.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paste_objects_on_blank_canvas(image, masks):\n",
    "    \"\"\"\n",
    "    Cut out each object defined by SAM2 masks and paste onto a blank canvas.\n",
    "    \n",
    "    Args:\n",
    "        image (np.ndarray): Original image in BGR or RGB (H x W x 3).\n",
    "        masks (list[dict]): List of masks from SAM2 with 'segmentation' and 'bbox' keys.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Blank canvas with all segmented objects pasted.\n",
    "    \"\"\"\n",
    "    # Prepare a blank canvas of the same dimensions and type as 'image'\n",
    "    height, width = image.shape[:2]\n",
    "    blank_canvas = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    \n",
    "    for mask_info in masks:\n",
    "        # Segmentation mask (boolean array the same size as the image)\n",
    "        seg = mask_info['segmentation']\n",
    "        \n",
    "        # BBox is in XYWH format. Convert to integer if needed.\n",
    "        x, y, w, h = map(int, mask_info['bbox'])\n",
    "        x2, y2 = x + w, y + h\n",
    "        \n",
    "        # Crop out the object region from the original image\n",
    "        object_region = image[y:y2, x:x2]\n",
    "        \n",
    "        # For convenience, get the corresponding region in the segmentation mask\n",
    "        seg_region = seg[y:y2, x:x2]\n",
    "        \n",
    "        # \"Paste\" the object onto the blank canvas where the mask is True\n",
    "        blank_canvas[y:y2, x:x2][seg_region] = object_region[seg_region]\n",
    "    \n",
    "    return blank_canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paste_objects_on_background_canvas(image, masks, background_image):\n",
    "    \"\"\"\n",
    "    Cut out each object defined by SAM2 masks and paste onto a background canvas.\n",
    "    \n",
    "    Args:\n",
    "        image (np.ndarray): Original image in BGR or RGB (H x W x 3).\n",
    "        masks (list[dict]): List of masks from SAM2 with 'segmentation' and 'bbox' keys.\n",
    "        background_image (PIL.Image.Image or np.ndarray): Background image in BGR or RGB (H x W x 3).\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Background canvas with all segmented objects pasted.\n",
    "    \"\"\"\n",
    "    # Convert background_image to a NumPy array if it's a PIL Image\n",
    "    if isinstance(background_image, Image.Image):\n",
    "        background_image = np.array(background_image)\n",
    "    \n",
    "    # Ensure the background image has the same dimensions as the original image\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    background_canvas = None\n",
    "    \n",
    "    if background_image.shape[:2] != (height, width):\n",
    "        background_canvas = cv2.resize(background_image, (width, height))\n",
    "    else:\n",
    "        background_canvas = background_image.copy()\n",
    "    \n",
    "    for mask_info in masks:\n",
    "        # Segmentation mask (boolean array the same size as the image)\n",
    "        seg = mask_info['segmentation']\n",
    "        \n",
    "        # BBox is in XYWH format. Convert to integer if needed.\n",
    "        x, y, w, h = map(int, mask_info['bbox'])\n",
    "        x2, y2 = x + w, y + h\n",
    "        \n",
    "        # Crop out the object region from the original image\n",
    "        object_region = image[y:y2, x:x2]\n",
    "        \n",
    "        # For convenience, get the corresponding region in the segmentation mask\n",
    "        seg_region = seg[y:y2, x:x2]\n",
    "        \n",
    "        # \"Paste\" the object onto the background canvas where the mask is True\n",
    "        background_canvas[y:y2, x:x2][seg_region] = object_region[seg_region]\n",
    "    \n",
    "    return background_canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Load an example image (modify path to your own image)\n",
    "# image_path = \"../inputs/surf_school_test_image.png\"\n",
    "# original_image = cv2.imread(image_path)  \n",
    "# # Ensure 'original_image' is loaded. If None, check the file path.\n",
    "\n",
    "# # For demonstration, we'll just return the image size\n",
    "# width, height, _ = original_image.shape\n",
    "# # Convert the image to bytes\n",
    "# _, img_encoded = cv2.imencode('.png', original_image)\n",
    "# img_byte_arr = img_encoded.tobytes()\n",
    "# # exrtact the image data url from the image byte array\n",
    "# image_data_url = await local_image_bytes_to_data_url(img_byte_arr)\n",
    "# # print(f\"Image data url:  {image_data_url}\")\n",
    "\n",
    "# # exrtact the background elements from the image\n",
    "# background_elements = await detect_background_elements(image_data_url)\n",
    "# print(f\"Background elements: {background_elements}\")\n",
    "\n",
    "# # Adjust the background elements to match the new canvas size\n",
    "# adjusted_background_elements = adjust_background_elements(background_elements, width, height)\n",
    "\n",
    "# # Draw the background elements on a new canvas\n",
    "# adjusted_background_elements_img_size  = adjusted_background_elements[\"image_size\"]\n",
    "# (new_width, new_height) = (adjusted_background_elements_img_size[\"width\"], adjusted_background_elements_img_size[\"height\"])\n",
    "# print(f\"New canvas size: {new_width} x {new_height}\")\n",
    "# background_canvas = draw_background_elements(None, adjusted_background_elements, {\"width\": new_width, \"height\": new_height})\n",
    "\n",
    "# # 2. Generate masks using SAM2\n",
    "# # For illustration, assume 'mask_generator_2' is already defined/configured.\n",
    "# mask_generator = SAM2AutomaticMaskGenerator(sam_model)\n",
    "# # mask_generator_2 = SAM2AutomaticMaskGenerator(\n",
    "# #     model=sam_model,\n",
    "# #     points_per_side=128,  # Increased for finer granularity in object detection.\n",
    "# #     points_per_batch=256,  # Increased to handle more points efficiently.\n",
    "# #     pred_iou_thresh=0.6,  # Slightly lowered to include more objects with less strict IoU.\n",
    "# #     stability_score_thresh=0.88,  # Balanced for both stability and inclusion.\n",
    "# #     stability_score_offset=0.6,  # Slightly reduced for broader stability score acceptance.\n",
    "# #     crop_n_layers=2,  # More cropping layers to capture complex hierarchies.\n",
    "# #     box_nms_thresh=0.6,  # Reduced to allow slightly overlapping detections.\n",
    "# #     crop_n_points_downscale_factor=1,  # No downscaling for precise object segmentation.\n",
    "# #     min_mask_region_area=10.0,  # Reduced to capture smaller objects like text or small graphics.\n",
    "# #     use_m2m=True,  # Enable mask-to-mask refinement for higher-quality results.\n",
    "# # )\n",
    "# # masks = mask_generator_2.generate(original_image)\n",
    "# masks = mask_generator.generate(original_image)\n",
    "\n",
    "# # 3. Paste all detected objects onto a blank canvas\n",
    "# canvas_with_objects = paste_objects_on_blank_canvas(original_image, masks)\n",
    "\n",
    "# # draw objects on the background canvas\n",
    "# background_canvas_with_objects = paste_objects_on_background_canvas(original_image, masks, background_canvas)\n",
    "\n",
    "# # 4. Show the result\n",
    "# # Option A: Use OpenCV (BGR format)\n",
    "# # cv2.imshow(\"Canvas with Objects\", canvas_with_objects)\n",
    "# # cv2.waitKey(0)\n",
    "# # cv2.destroyAllWindows()\n",
    "\n",
    "# # oringal image\n",
    "# cv2.imshow(\"original_image\", original_image)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "# # plt.imshow(original_image, cv2.COLOR_BGR2RGB)\n",
    "# # plt.title(\"Original image\")\n",
    "# # plt.axis(\"off\")\n",
    "# # plt.show()\n",
    "\n",
    "# # Option B: Use Matplotlib (assumes RGB format)\n",
    "# # Convert from BGR to RGB for correct color rendering in Matplotlib\n",
    "# # canvas_rgb = cv02.cvtColor(canvas_with_objects, cv2.COLOR_BGR2RGB)\n",
    "# cv2.imshow(\"canvas with objects\", canvas_with_objects)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "# # Convert from BGR to RGB for correct color rendering in Matplotlib\n",
    "# # background_canvas_rgb = cv2.cvtColor(background_canvas_with_objects, cv2.COLOR_BGR2RGB)\n",
    "# cv2.imshow(\"background canvas with objects\", background_canvas_with_objects)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text response for background elements: ```json\n",
      "{\n",
      "  \"image_size\": {\n",
      "    \"width\": 1085,\n",
      "    \"height\": 768\n",
      "  },\n",
      "  \"background_elements\": [\n",
      "    {\n",
      "      \"type\": \"solid_color\",\n",
      "      \"bounding_box\": {\n",
      "        \"x\": 0,\n",
      "        \"y\": 0,\n",
      "        \"width\": 1085,\n",
      "        \"height\": 768\n",
      "      },\n",
      "      \"color\": \"#2F3E46\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"shape\",\n",
      "      \"bounding_box\": {\n",
      "        \"x\": 492,\n",
      "        \"y\": 400,\n",
      "        \"width\": 100,\n",
      "        \"height\": 50\n",
      "      },\n",
      "      \"color\": \"#FFD166\",\n",
      "      \"shape_type\": \"semi_circle\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"horizontal_line\",\n",
      "      \"bounding_box\": {\n",
      "        \"x\": 100,\n",
      "        \"y\": 600,\n",
      "        \"width\": 885,\n",
      "        \"height\": 5\n",
      "      },\n",
      "      \"color\": \"#FFD166\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"horizontal_line\",\n",
      "      \"bounding_box\": {\n",
      "        \"x\": 100,\n",
      "        \"y\": 650,\n",
      "        \"width\": 885,\n",
      "        \"height\": 5\n",
      "      },\n",
      "      \"color\": \"#FFD166\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "extracted_text_resp is not a valid JSON string\n",
      "Background elements JSON: {'image_size': {'width': 1085, 'height': 768}, 'background_elements': [{'type': 'solid_color', 'bounding_box': {'x': 0, 'y': 0, 'width': 1085, 'height': 768}, 'color': '#2F3E46'}, {'type': 'shape', 'bounding_box': {'x': 492, 'y': 400, 'width': 100, 'height': 50}, 'color': '#FFD166', 'shape_type': 'semi_circle'}, {'type': 'horizontal_line', 'bounding_box': {'x': 100, 'y': 600, 'width': 885, 'height': 5}, 'color': '#FFD166'}, {'type': 'horizontal_line', 'bounding_box': {'x': 100, 'y': 650, 'width': 885, 'height': 5}, 'color': '#FFD166'}]}\n",
      "Background elements: {'image_size': {'width': 1085, 'height': 768}, 'background_elements': [{'type': 'solid_color', 'bounding_box': {'x': 0, 'y': 0, 'width': 1085, 'height': 768}, 'color': '#2F3E46'}, {'type': 'shape', 'bounding_box': {'x': 492, 'y': 400, 'width': 100, 'height': 50}, 'color': '#FFD166', 'shape_type': 'semi_circle'}, {'type': 'horizontal_line', 'bounding_box': {'x': 100, 'y': 600, 'width': 885, 'height': 5}, 'color': '#FFD166'}, {'type': 'horizontal_line', 'bounding_box': {'x': 100, 'y': 650, 'width': 885, 'height': 5}, 'color': '#FFD166'}]}\n",
      "Element: {'type': 'solid_color', 'bounding_box': {'x': 0, 'y': 0, 'width': 1085, 'height': 768}, 'color': '#2F3E46'}\n",
      "Element: {'type': 'shape', 'bounding_box': {'x': 492, 'y': 400, 'width': 100, 'height': 50}, 'color': '#FFD166', 'shape_type': 'semi_circle'}\n",
      "Element: {'type': 'horizontal_line', 'bounding_box': {'x': 100, 'y': 600, 'width': 885, 'height': 5}, 'color': '#FFD166'}\n",
      "Element: {'type': 'horizontal_line', 'bounding_box': {'x': 100, 'y': 650, 'width': 885, 'height': 5}, 'color': '#FFD166'}\n",
      "New canvas size: 4960 x 3507\n",
      "background_elements: {'image_size': {'width': 4960, 'height': 3507}, 'background_elements': [{'color': '#2F3E46', 'type': 'solid_color', 'bounding_box': {'x': 0.0, 'y': 0.0, 'width': 4960.0, 'height': 3507.0}}, {'color': '#FFD166', 'type': 'shape', 'bounding_box': {'x': 2249.142857142857, 'y': 1826.5625, 'width': 457.1428571428571, 'height': 228.3203125}}, {'color': '#FFD166', 'type': 'horizontal_line', 'bounding_box': {'x': 457.1428571428571, 'y': 2739.84375, 'width': 4045.7142857142853, 'height': 22.83203125}}, {'color': '#FFD166', 'type': 'horizontal_line', 'bounding_box': {'x': 457.1428571428571, 'y': 2968.1640625, 'width': 4045.7142857142853, 'height': 22.83203125}}]}\n",
      "Element: {'color': '#2F3E46', 'type': 'solid_color', 'bounding_box': {'x': 0.0, 'y': 0.0, 'width': 4960.0, 'height': 3507.0}}\n",
      "Element type: solid_color\n",
      "Element: {'color': '#FFD166', 'type': 'shape', 'bounding_box': {'x': 2249.142857142857, 'y': 1826.5625, 'width': 457.1428571428571, 'height': 228.3203125}}\n",
      "Element type: shape\n",
      "Element: {'color': '#FFD166', 'type': 'horizontal_line', 'bounding_box': {'x': 457.1428571428571, 'y': 2739.84375, 'width': 4045.7142857142853, 'height': 22.83203125}}\n",
      "Element type: horizontal_line\n",
      "Element: {'color': '#FFD166', 'type': 'horizontal_line', 'bounding_box': {'x': 457.1428571428571, 'y': 2968.1640625, 'width': 4045.7142857142853, 'height': 22.83203125}}\n",
      "Element type: horizontal_line\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anupamgadobe.com/Work/sam2/sam2/sam2_image_predictor.py:431: UserWarning: cannot import name '_C' from 'sam2' (/Users/anupamgadobe.com/Work/sam2/sam2/__init__.py)\n",
      "\n",
      "Skipping the post-processing step due to the error above. You can still use SAM 2 and it's OK to ignore the error above, although some post-processing functionality may be limited (which doesn't affect the results in most cases; see https://github.com/facebookresearch/sam2/blob/main/INSTALL.md).\n",
      "  masks = self._transforms.postprocess_masks(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 1. Load an example image (modify path to your own image)\n",
    "image_path = \"../inputs/candel.png\"\n",
    "original_image = cv2.imread(image_path)  \n",
    "# Ensure 'original_image' is loaded. If None, check the file path.\n",
    "\n",
    "# For demonstration, we'll just return the image size\n",
    "width, height, _ = original_image.shape\n",
    "# Convert the image to bytes\n",
    "_, img_encoded = cv2.imencode('.png', original_image)\n",
    "img_byte_arr = img_encoded.tobytes()\n",
    "# exrtact the image data url from the image byte array\n",
    "image_data_url = await local_image_bytes_to_data_url(img_byte_arr)\n",
    "# print(f\"Image data url:  {image_data_url}\")\n",
    "\n",
    "# exrtact the background elements from the image\n",
    "background_elements = await detect_background_elements(image_data_url)\n",
    "print(f\"Background elements: {background_elements}\")\n",
    "\n",
    "# Adjust the background elements to match the new canvas size\n",
    "adjusted_background_elements = adjust_background_elements(background_elements, width, height)\n",
    "\n",
    "# Draw the background elements on a new canvas\n",
    "adjusted_background_elements_img_size  = adjusted_background_elements[\"image_size\"]\n",
    "(new_width, new_height) = (adjusted_background_elements_img_size[\"width\"], adjusted_background_elements_img_size[\"height\"])\n",
    "print(f\"New canvas size: {new_width} x {new_height}\")\n",
    "background_canvas = draw_background_elements(None, adjusted_background_elements, {\"width\": new_width, \"height\": new_height})\n",
    "\n",
    "# 2. Generate masks using SAM2\n",
    "# For illustration, assume 'mask_generator_2' is already defined/configured.\n",
    "# mask_generator = SAM2AutomaticMaskGenerator(sam_model)\n",
    "mask_generator_2 = SAM2AutomaticMaskGenerator(\n",
    "    model=sam_model,\n",
    "    points_per_side=128,  # Increased for finer granularity in object detection.\n",
    "    points_per_batch=256,  # Increased to handle more points efficiently.\n",
    "    min_mask_region_area=10.0,  # Reduced to capture smaller objects like text or small graphics.\n",
    ")\n",
    "masks = mask_generator_2.generate(original_image)\n",
    "# masks = mask_generator.generate(original_image)\n",
    "\n",
    "# 3. Paste all detected objects onto a blank canvas\n",
    "canvas_with_objects = paste_objects_on_blank_canvas(original_image, masks)\n",
    "\n",
    "# draw objects on the background canvas\n",
    "background_canvas_with_objects = paste_objects_on_background_canvas(original_image, masks, background_canvas)\n",
    "\n",
    "# 4. Show the result\n",
    "# Option A: Use OpenCV (BGR format)\n",
    "# cv2.imshow(\"Canvas with Objects\", canvas_with_objects)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "# oringal image\n",
    "display(original_image)\n",
    "# cv2.imshow(\"original_image\", original_image)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "# plt.imshow(original_image, cv2.COLOR_BGR2RGB)\n",
    "# plt.title(\"Original image\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "# Option B: Use Matplotlib (assumes RGB format)\n",
    "# Convert from BGR to RGB for correct color rendering in Matplotlib\n",
    "# canvas_rgb = cv02.cvtColor(canvas_with_objects, cv2.COLOR_BGR2RGB)\n",
    "cv2.imshow(\"canvas with objects\", canvas_with_objects)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Convert from BGR to RGB for correct color rendering in Matplotlib\n",
    "# background_canvas_rgb = cv2.cvtColor(background_canvas_with_objects, cv2.COLOR_BGR2RGB)\n",
    "cv2.imshow(\"background canvas with objects\", background_canvas_with_objects)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
